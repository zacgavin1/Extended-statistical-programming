key <- key[(length(key) - mlag + 1):length(key)]
}
#Here we create an empty vector called "all_next_words". This is all the possible words that can
#come next.This is our "raffle drum" as we drop a "ticket" for the word that shows up into the vector
all_next_words <- c()
#This for loop below allows us to try out all different lengths. If the input key has 4 words
#the loop will run 4 times: The first time using the full 4-word context. The second time it
#will search using the last 3 words of the context. The third time using the last 2 words.
#Finally using the last word.
for (i in 1:length(key)) {
current_key <- key[i:length(key)]
context_len <- length(current_key)
cols_to_match <- (mlag - context_len + 1):mlag
#If length(key) = 4. An example for the above code will be, sps i = 2. Current_key <- key[2,4]
#Context_len = 3 which is the number of words remaining in key. cols_to_match will be (4-3+1):4
# which is just 2:4 as required because we are going from 2:4.
#For this command from the PDF below, check WA gc
#An example would be sps we want c(1, 2, 3) and we find that pattern in rows 5, 200 and 512
#Then matching_rows will be c(5, 200, 512)
ii <- colSums(!(t(M[, cols_to_match, drop=FALSE])==current_key))
matching_rows <- which(ii == 0)
#Here we collect the "prizes" for our spsd "raffle" This code only runs if we found any matches
#The first line in the if statement looks at the matching rows and grabs the value from the 5th column
#The second line takes the tokens and adds it to our "raffle drum", na.omit cleans out any NA's (rare words)
#i.e. if the sentence "to be," was followed by "alas" (token NA) once and "my" (token 15) twice,
#This step would add 15 and 15 into all_next_words vector and exclude "alas"
if (length(matching_rows) > 0) {
next_words_found <- M[matching_rows, mlag + 1]
all_next_words <- c(all_next_words, na.omit(next_words_found))
}
}
#The first part of the if the statement samples one "ticket" from our "raffle drum", if "all_next_words"
#has any tickets in it. If a token was found more often -> has more tickets -> higher prob of being selected
#the else part of the if statement just gives up and chooses a random word from the entire book
#The entire book here is 'M1' which is the tokenised vector
if (length(all_next_words) > 0) {
next_token <- sample(all_next_words, 1)
} else {
next_token <- sample(na.omit(M1), 1)
}
#Below we return the single token that was chosen which is the model's final answer for the word that
#comes next.
return(next_token)
}
######## Question 8 ###########
#M2 here is the vector of all the words from the vector a excluding punctuation now as well
M2 <- a[grepl("[A-Za-z]", a)]
#start_word is a random word chosen from M2
#We repeat finding this start_word until we don't have an N/A (low prob anyway)
repeat {
start_word <- sample(M2, 1)
start_token <- match(start_word, b)
if (!is.na(start_token)) {
break
}
}
#here we get our start_word and the token for it
#we get rid of N/A's as we don't have those rare words in tokenised form
print(start_word)
print(start_token)
######## Question 9 ###########
#Now we will simulate from the model until a full stop is reached.Then we can convert the generated tokens back to words and print them nicely
########################
# sentence generator
sentence <- function(start_token) {
#initiate loop
token.v <- start_token
start_word <- b[start_token]
output <- c(start_word)
#generate first four words
for(i in 1:mlag) {
nw.token <- next.word(token.v, M, M1)
token.v <- append(token.v, nw.token)
nw <- b[nw.token]
output <- append(output, nw)
if (nw == ".") break
#print(token.v) #uncomment to see tokens inputted + token output
#print(nw)
}
#generate if longer than four words
while(nw != ".") {
token.v <- token.v[2:1+mlag] #use last four tokens to generate next token
nw.token <- next.word(token.v, M, M1)
nw <- b[nw.token]
token.v <- append(token.v, nw.token)
output <- append(output, nw)
#print(token.v)
#print(nw)
}
cat(output)
}
# generate random word from text
generate_word <- function(word_list) {
repeat {
start_word <- sample(word_list, 1)
start_token <- match(start_word, b)
if (!is.na(start_token)) {
break
}
}
return(c(start_word, start_token))
}
start_token <- as.numeric(generate_word(M2)[2])
sentence(start_token)
#setwd("Extended-statistical-programming") ## comment out of submitted
setwd("C:/Users/shaeh/Desktop/edinburgh-notes/Extended-Statistical-Programming/project-ESP/Extended-statistical-programming") ## comment out of submitted
#setwd("C:/Users/Brandon Causing/Downloads/Extended Statistical Programming/Extended-statistical-programming")
a <- scan("shakespeare.txt",what="character",skip=83,nlines=196043-83,
fileEncoding="UTF-8")
####### START OF PREPROCESSING ###############
## (a) identify and remove stage directions
# First we manually fix all bracketing errors in the text
a[463343] <- "So"    # The original text was erroneously "[So"
a[322751] <- "Rest.]" # missing end of direction
direction_starts = grep("[", a, fixed = TRUE) # find where stage directions start
direction_ends <- c()
for (direction in direction_starts) {
close_brackets <- grep("]", a[direction:(direction+100)]); # this gets ALL close brackets following an open bracket
direction_ends <- append(direction_ends, close_brackets[1]) # and then we choose the first one after each [
}
direction_indexes <- c() # to hold positions of stage directions
# the following loop may be able to be done more nicely by some vector operation
for (i in 1:length(direction_starts)){
direction_indexes <- append(direction_indexes, direction_starts[i]:(direction_starts[i]+direction_ends[i]-1)) ;
}
a <- a[-direction_indexes] # removes all stage directions
## (b) Removing character names and arabic numerals #####
# note: some roman numeral I's are likely left in
a <- a[-which((toupper(a)==a) & (a!= "I") & (a != "A") &(a != "O")) ]
## (c) removing _ and - from words
a <- gsub("-", "", a ) # it may be better to split the words rather than combining them
a <- gsub("_", "", a )
## (d) isolating punctuation
# this task is similar to the one from notes
split_punc <- function(v, marks){
regex = paste(marks, collapse="|") # This contains . which will give problems
i_punc <- which(grepl(regex, v)) # using regex to see where elements of marks are
vs <- rep("", length(v)+length(i_punc)) # initialise new vector of req length
ips <- i_punc+ 1:length(i_punc) # i assume here all punctuation is at end of words
vs[ips]<- substr(v[i_punc], nchar(v[i_punc]), nchar(v[i_punc]))
vs[ips-1] <- substr(v[i_punc], 1, nchar(v[i_punc])-1) # words before puncs
vs[-(c(ips, ips-1))] <- v[-i_punc] # all other words
vs
}
# (e) Splitting up the punctuation and the words
a <- split_punc(a, c(",","\\.",";","!",":","\\?"))    #need to use \\ before symbols that have regex meaning like .
# (f) making 'a' lower case
a <- tolower(a)
########### END OF THE PREPROCESSING ############
######## Question 5 ############
# (a) Using 'unique' to find the vector of unique words in 'a'. We store this in 'uniqueWords'
uniqueWords <- unique(a)
# (b) Creating a numerical index vector 'e' by matching each word in 'a' to its position in 'uniqueWords'
e <- match(a, uniqueWords)
# (c) Using 'tabulate' on the index vector 'e' to count the occurrences of each word.
occurences <- tabulate(e)
# Assigning the actual words from 'uniqueWords' as names to the numerical counts.
names(occurences) <- uniqueWords
# (d) Sort the named vector of occurrences in descending order to find the most frequent words
occurences <- sort(occurences, decreasing=TRUE)
# We extract the names of the top 1000 words to create 'b'
b <- names(occurences[1:1000])
######## Question 6 ###########
n    <- length(a) # note this is the same length as M1
mlag <- 4
# (a)
M1 <- match(a,b) # now have a tokenised version of a. In instructions he calls this M1
# (b)
M <- M1[1:(n-mlag)]
for (col in 2:(mlag+1)){ # loop appends shifted copies of M1 to the right of M
M <- cbind(M, M1[col:(n-mlag-1 + col)])
}
####### Question 7 ###########
#we define the function next.word below. key represents the sequence of word tokens we are using
#as context to predict the next word. M is the matrix of all 5-word sequences from Shakespeare
#M1 is the tokenised version of the whole text
#w is the mixture weights
next.word <- function(key, M, M1, w=rep(1,ncol(M)-1)){
#mlag is set to 4 as M has 5 columns.
mlag <- ncol(M) - 1
#Below, we make sure the key is not too long. The model is built for only 4 words being mlag here.
#Sps key was c(10, 20, 30, 40, 50, 60) this would become c(30, 40, 50, 60)
if (length(key) > mlag) {
key <- key[(length(key) - mlag + 1):length(key)]
}
#Here we create an empty vector called "all_next_words". This is all the possible words that can
#come next.This is our "raffle drum" as we drop a "ticket" for the word that shows up into the vector
all_next_words <- c()
length_i <- c() # no. of next words in iteration i
#This for loop below allows us to try out all different lengths. If the input key has 4 words
#the loop will run 4 times: The first time using the full 4-word context. The second time it
#will search using the last 3 words of the context. The third time using the last 2 words.
#Finally using the last word.
for (i in 1:length(key)) {
current_key <- key[i:length(key)]
context_len <- length(current_key)
cols_to_match <- (mlag - context_len + 1):mlag
#If length(key) = 4. An example for the above code will be, sps i = 2. Current_key <- key[2,4]
#Context_len = 3 which is the number of words remaining in key. cols_to_match will be (4-3+1):4
# which is just 2:4 as required because we are going from 2:4.
#For this command from the PDF below, check WA gc
#An example would be sps we want c(1, 2, 3) and we find that pattern in rows 5, 200 and 512
#Then matching_rows will be c(5, 200, 512)
ii <- colSums(!(t(M[, cols_to_match, drop=FALSE])==current_key))
matching_rows <- which(ii == 0)
#Here we collect the "prizes" for our spsd "raffle" This code only runs if we found any matches
#The first line in the if statement looks at the matching rows and grabs the value from the 5th column
#The second line takes the tokens and adds it to our "raffle drum", na.omit cleans out any NA's (rare words)
#i.e. if the sentence "to be," was followed by "alas" (token NA) once and "my" (token 15) twice,
#This step would add 15 and 15 into all_next_words vector and exclude "alas"
if (length(matching_rows) > 0) {
next_words_found <- M[matching_rows, mlag + 1]
all_next_words <- c(all_next_words, na.omit(next_words_found))
length_i <- c(length_i, length(na.omit(next_words_found)))
} else {
length_i <- c(length_i, 0) # 0 length_i = 0 if no matches
}
print(c("length_i:", length_i))
}
# table of next words with their corresponding weights
next_words_table <- cbind(all_next_words,
weights = rep(w[1:length(key)], length_i))
# uncomment these to see what's happening in each iteration
print(c("length of table:", length(next_words_table[,1])))
print(c("table:"))
print(head(next_words_table))
#The first part of the if the statement samples one "ticket" from our "raffle drum", if "all_next_words"
#has any tickets in it. If a token was found more often -> has more tickets -> higher prob of being selected
#the else part of the if statement just gives up and chooses a random word from the entire book
#The entire book here is 'M1' which is the tokenised vector
if (length(next_words_table[,1]) > 0) {
next_token <- sample(next_words_table[,1], 1, prob = next_words_table[, "weights"])
#print("true") # uncomment to see which path is taken
} else {
next_token <- sample(na.omit(M1), 1)
#print("false")
}
#Below we return the single token that was chosen which is the model's final answer for the word that
#comes next.
return(next_token)
}
######## Question 8 ###########
#M2 here is the vector of all the words from the vector a excluding punctuation now as well
M2 <- a[grepl("[A-Za-z]", a)]
#start_word is a random word chosen from M2
#We repeat finding this start_word until we don't have an N/A (low prob anyway)
repeat {
start_word <- sample(M2, 1)
start_token <- match(start_word, b)
if (!is.na(start_token)) {
break
}
}
#here we get our start_word and the token for it
#we get rid of N/A's as we don't have those rare words in tokenised form
print(start_word)
print(start_token)
######## Question 9 ###########
#Now we will simulate from the model until a full stop is reached.Then we can convert the generated tokens back to words and print them nicely
########################
# sentence generator
sentence <- function(start_token, w) {
#initiate loop
token.v <- start_token
start_word <- b[start_token]
output <- c(start_word)
#generate first four words
for(i in 1:mlag) {
nw.token <- next.word(token.v, M, M1, w)
token.v <- append(token.v, nw.token)
nw <- b[nw.token]
output <- append(output, nw)
if (nw == ".") break
#print(token.v) #uncomment to see tokens inputted + token output
#print(nw)
}
#generate if longer than four words
while(nw != ".") {
token.v <- token.v[2:(1+mlag)] #use last four tokens to generate next token
nw.token <- next.word(token.v, M, M1, w)
nw <- b[nw.token]
token.v <- append(token.v, nw.token)
output <- append(output, nw)
#print(token.v)
#print(nw)
}
cat(output)
}
# generate random word from text
generate_word <- function(word_list) {
repeat {
start_word <- sample(word_list, 1)
start_token <- match(start_word, b)
if (!is.na(start_token)) {
break
}
}
return(c(start_word, start_token))
}
start_token <- as.numeric(generate_word(M2)[2])
sentence(start_token, w = c(1, 1, 1, 1))
sentence(start_token, w = c(1000, 100, 10, 1))
#setwd("Extended-statistical-programming") ## comment out of submitted
setwd("C:/Users/shaeh/Desktop/edinburgh-notes/Extended-Statistical-Programming/project-ESP/Extended-statistical-programming")
#setwd("C:/Users/Brandon Causing/Downloads/Extended Statistical Programming/Extended-statistical-programming")
## GENERAL DESCRIPTION
# The project aim is to make a "Shakespeare text generator" using a Markov chain idea.
# The first section pre-processes the text, mainly dealing with stage directions and punctuation.
# The next token prediction is based off the idea of feeding in a key of some length, and finding
# all matches of that key in the text, where we can then sample from all the words that follow the key
# in the text. When we predict next words, we mix together matches from the previous 1, 2,...,n
# words, for some chosen n, to ensure we have some matches.
# Finally we feed in a starting word, and then repeatedly sample to produce a full sentence.
a <- scan("shakespeare.txt",what="character",skip=83,nlines=196043-83,
fileEncoding="UTF-8")
###########################################################
######### ------- START OF PREPROCESSING ------- ##########
###########################################################
# We begin by removing parts of the text that are not part of the literature itself (i.e
# stage directions, character names), and putting the text into a tokenisable form. We do
# this to generate proper sentences based only on the dialogue and with accurate sampling
# probabilities.
#### --- Identify and remove stage directions --- ####
# First we manually fix all bracketing errors in the text
a[463343] <- "So"    # The original text was erroneously "[So"
a[322751] <- "Rest.]" # missing end of direction
# Find starts of stage directions
direction_starts = grep("[", a, fixed = TRUE)
# Find ends of stage directions (relative to the start)
direction_ends <- c()
for (direction in direction_starts) {
# get all closed brackets in the 100 words following an open bracket
close_brackets <- grep("]", a[direction:(direction+100)])
# and then we choose the first appearance after each "["
direction_ends <- append(direction_ends, close_brackets[1])
}
# Collect indices of all words within stage directions
direction_indexes <- c() # to hold positions of stage directions
for (i in 1:length(direction_starts)) {
dir_start_to_end <- direction_starts[i]:(direction_starts[i]+direction_ends[i]-1)
direction_indexes <- append(direction_indexes, dir_start_to_end)
}
# Remove all stage directions
a <- a[-direction_indexes]
#### --- Removing character names and Arabic numerals --- #####
# note: some roman numeral I's are likely left in
a <- a[-which((toupper(a)==a) & (a!= "I") & (a != "A") &(a != "O")) ]
#### --- Removing _ and - from words --- ####
# we combine words at hyphens rather than split them, as it made sense in more cases in the text
a <- gsub("-", "", a )
a <- gsub("_", "", a )
#### --- Isolating punctuation --- ####
# split_punc() splits words and punctuation into individual strings
# * v = string vector
# * marks = string vector of characters (symbols w/ regex meaning preceded by \\)
split_punc <- function(v, marks){
regex = paste(marks, collapse="|")
i_punc <- which(grepl(regex, v)) # indexes of a where there is punctuation
vs <- rep("", length(v)+length(i_punc)) # initialise new vector of req length
ips <- i_punc+ 1:length(i_punc) # assume here all punctuation is at end of words
vs[ips]<- substr(v[i_punc], nchar(v[i_punc]), nchar(v[i_punc])) # puncs
vs[ips-1] <- substr(v[i_punc], 1, nchar(v[i_punc])-1) # words before puncs
vs[-(c(ips, ips-1))] <- v[-i_punc] # all other words
vs
}
#### --- Call function to split up the puncs and the words --- ####
# need to use \\ before symbols that have regex meaning like "."
a <- split_punc(a, marks = c(",","\\.",";","!",":","\\?"))
#### --- Make whole text lower case --- ####
# for simplicity in use
a <- tolower(a)
###########################################################
########### ------ END OF PREPROCESSING ------ ############
###########################################################
###########################################################
######### ------ SETTING UP THE GENERATOR ------- #########
###########################################################
# We proceed by ranking words by usage and contextualising strings of words to logically
# pick each following word, given a starting point.
######## ----- Find Most Common Words ----- ############
words <- unique(a) # set of all words used
e <- match(a,words) # where each element of a occurs first in a
occurences <- tabulate(e) # count number of times each word shows up
names(occurences) <- words  # list of occurrences, with entry names the words
occurences <- sort(occurences, decreasing=TRUE)
b <- names(occurences[1:1000])  # now have the top 1000 words
###### ----- Set Up Word Contextualisation ----- #######
n    <- length(a)
mlag <- 4
# tokenize a; words not in b (most common words) are tokenenised as NA
M1 <- match(a,b)
# -- rows of M are all (mlag+1) length sequences of (tokenised) words from text
# -- match input string to rows of M to find set of possible next-words
M <- M1[1:(n-mlag)]
for (col in 2:(mlag+1)){ # loop appends shifted copies of M1 to the right of M
M <- cbind(M, M1[col:(n-mlag-1 + col)])
}
####### ----- Pick the Next Word ----- ###########
# pick a next-word based on key (sequence of tokens), M, M1, w (weights)
next.word <- function(key, M, M1, w=rep(1,ncol(M)-1)){
###### -- 1 - Set-Up -- ######
mlag <- ncol(M) - 1 # define mlag in terms of function argument M
if (length(key) > mlag) { # only use the last mlag tokens
key <- key[(length(key) - mlag + 1):length(key)]
}
all_next_words <- c() # list of next-words to sample from
length_i <- c() # no. of possible next-words in iteration i
##### -- 2 - Search for Next Words -- ######
# find key match of length mlag, mlag - 1, ... , 1
for (i in 1:length(key)) {
### - 2a - Pick columns to match key to - ###
current_key <- key[i:length(key)] # use last mlag - i + 1 tokens of key
context_len <- length(current_key)
cols_to_match <- (mlag - context_len + 1):mlag # which cols in M to match
### - 2b - Find matches - ###
# return F if key[j] matches M[k, j] for some row k of M; T otherwise
ii <- colSums(!(t(M[, cols_to_match, drop=FALSE])==current_key))
# if sum of components in a row = 0, entire key matches
matching_rows <- which(ii == 0)
### - 2c - Store next-words and no. of next-words - ###
if (length(matching_rows) > 0) { # if there is a matching row
# get last column of M (possible next-words) for all matching rows
next_words_found <- M[matching_rows, mlag + 1]
# collect found next-words, omitting rare words (NA's)
all_next_words <- c(all_next_words, na.omit(next_words_found))
# store no. of (non-rare) next-words found in iteration i (length_i[i])
length_i <- c(length_i, length(na.omit(next_words_found)))
} else { # if there is not a matching row, length_i[i] = 0
length_i <- c(length_i, 0)
}
} # end for-loop
#### -- 3 - Assign Weights -- ####
# assign weights to next-words corresponding to length of matched string
# -- vectorised form of rep is used here
# -- weights = w_i/n_i
# -- n_i = no. of matches found with context length mlag-i+1
weights <- rep(w[1:length(key)]/length_i, length_i)
next_words_table <- cbind(all_next_words, weights = weights)
#### -- 4 - Pick a Next-Word -- ####
if (nrow(next_words_table) > 0) { # if there are next-words found
# sample from all possible next-words w/ assigned prob. weights
next_token <- sample(next_words_table[ , "all_next_words"],
1,
prob = next_words_table[ , "weights"])
} else { # if no next-words are found
next_token <- sample(na.omit(M1), 1) # sample from all words in text
}
#### -- 5 - Output -- ####
return(next_token)
}
###########################################################
############ ------ SENTENCE GENERATOR ------ #############
###########################################################
# Finally, we are able to randomly generate Shakespearean dialogue.
####### ----- Generate a Starting Word ----- ##########
# remove punctuation to make list exclusively of words to sample from
M2 <- a[grepl("[A-Za-z]", a)]
# sample token of any (common) word from text
generate_word <- function(word_list, b_match = b) {
start_tokens <- match(word_list, b_match)
start_token <- sample(start_tokens[!is.na(start_tokens)], 1)
return(start_token)
}
# generate the start word
start_token <- generate_word(M2)
######## ------ Generate a Sentence ------ ###########
# sentence generator
# generate token based on previous token(s) until full stop is generated
sentence <- function(start_token,
M.seq = M, M1.tokens = M1, b_match = b,
w = rep(1, ncol(M) - 1)) {
mlag <- ncol(M) - 1
# initialize loop with start token
token.v <- start_token
start_word <- b_match[start_token]
output_list <- c(start_word)
# generate first mlag words
for(i in 1:mlag) {
nw.token <- next.word(token.v, M.seq, M1.tokens, w)
token.v <- append(token.v, nw.token)
nw <- b_match[nw.token]
output_list <- append(output_list, nw)
if (nw == ".") break
}
# generate word if start token longer than mlag words
while(nw != ".") {
token.v <- token.v[2:(1+mlag)] # use last mlag - 1 tokens
nw.token <- next.word(token.v, M.seq, M1.tokens, w)
nw <- b_match[nw.token]
token.v <- append(token.v, nw.token)
output_list <- append(output_list, nw)
}
# generate sentence (with punctuation collapsed onto the previous word)
output <- c()
for (i in 1:length(output_list)) {
if (grepl("[A-Za-z]", output_list[i]) == TRUE) { #if string contains letter
output <- append(output, output_list[i])
output <- paste(output, collapse = " ")
} else {
output <- append(output, output_list[i])
output <- paste(output, collapse = "")
}
}
cat(output)
}
#### ---- Make a sentence ---- ####
# using default weights
sentence(start_token)
# heavier weights on longer matches
# -- more likely to produce more coherent sentences
# -- more likely to be shorter
sentence(start_token, w = c(1000, 100, 10, 1))
